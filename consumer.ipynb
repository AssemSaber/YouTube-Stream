{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,split\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_TOPIC = \"lastStream\"\n",
    "KAFKA_SERVER = \"localhost:9092\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming YouTubeLive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer=KafkaConsumer(KAFKA_TOPIC,bootstrap_servers=['localhost:9092'],auto_offset_reset='latest',api_version=(0,10))  # read data from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostName='localhost'\n",
    "port=3306\n",
    "database='streamdb'\n",
    "table='youtubelive'\n",
    "url=f'jdbc:mysql://{hostName}:{port}/{database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     df = spark.write.jdbc(url=url, table=table, properties=properties)\n",
    "# except Exception as e:\n",
    "#     print(f'Error : {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "counter=0\n",
    "\n",
    "for msg in consumer:\n",
    "\n",
    "    # data.append(msg.value)\n",
    "    # print(msg.value)                                                                      \n",
    "    value = msg.value.decode('utf-8') if isinstance(msg.value, bytes) else msg.value  # decode data\n",
    "    splited_values=value.split(',')\n",
    "    data.append(tuple(splited_values))\n",
    "    # print(keys)\n",
    "\n",
    "    if counter== 10: \n",
    "        # Taking data as string and spliting it into columns\n",
    "        # df.printSchema()\n",
    "        columns = [\n",
    "            'Video_ID',\n",
    "            'Channel_Name',\n",
    "            'Country',\n",
    "            'Category',\n",
    "            'Comments',\n",
    "            'Subscribers',\n",
    "            'views_video',\n",
    "            'likes',\n",
    "            'date_live'       \n",
    "           ]\n",
    "        df = spark.createDataFrame(data,schema=columns)\n",
    "\n",
    "        # dfColumns=df.withColumn('spiltedColumns',split(df[\"value\"],\",\"))\n",
    "        # dataLive=dfColumns.selectExpr(\n",
    "        # 'spiltedColumns[0] as Video_ID',\n",
    "        # 'spiltedColumns[1] as Channel_Name',\n",
    "        # 'spiltedColumns[2] as Country',\n",
    "        # 'spiltedColumns[3] as Category',\n",
    "        # 'cast(spiltedColumns[4] as int) Comments',\n",
    "        # 'cast(spiltedColumns[5] as int) as Subscribers',\n",
    "        # 'cast(spiltedColumns[6] as int) as views_video',\n",
    "        # 'cast(spiltedColumns[7] as int)as likes',\n",
    "        # 'to_date(spiltedColumns[8], \"yyyy-MM-dd\") as date_live'\n",
    "        # )\n",
    "        written = df.write.jdbc(url=url, table=table, properties=properties,mode='append')\n",
    "        data=[]\n",
    "        counter=0\n",
    "\n",
    "        \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.readStream.format('kafka') \\\n",
    "#     .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "#     .option('subscribe', 'streamData') \\\n",
    "#     .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = spark.writeStream.format('console').outMode('append').trigger(timeProcessing='5 seconds')\n",
    "# query=writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
